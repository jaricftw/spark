package spark.streaming

import spark.streaming.dstream._
import StreamingContext._
//import Time._

import spark.{RDD, Logging}
import spark.storage.StorageLevel

import scala.collection.mutable.ArrayBuffer
import scala.collection.mutable.HashMap

import java.io.{ObjectInputStream, IOException, ObjectOutputStream}

import org.apache.hadoop.fs.Path
import org.apache.hadoop.conf.Configuration

/**
 * A Discretized Stream (DStream), the basic abstraction in Spark Streaming, is a continuous
 * sequence of RDDs (of the same type) representing a continuous stream of data (see [[spark.RDD]]
 * for more details on RDDs). DStreams can either be created from live data (such as, data from
 * HDFS, Kafka or Flume) or it can be generated by transformation existing DStreams using operations
 * such as `map`, `window` and `reduceByKeyAndWindow`. While a Spark Streaming program is running, each
 * DStream periodically generates a RDD, either from live data or by transforming the RDD generated
 * by a parent DStream.
 *
 * This class contains the basic operations available on all DStreams, such as `map`, `filter` and
 * `window`. In addition, [[spark.streaming.PairDStreamFunctions]] contains operations available
 * only on DStreams of key-value pairs, such as `groupByKeyAndWindow` and `join`. These operations
 * are automatically available on any DStream of the right type (e.g., DStream[(Int, Int)] through
 * implicit conversions when `spark.streaming.StreamingContext._` is imported.
 *
 * DStreams internally is characterized by a few basic properties:
 *  - A list of other DStreams that the DStream depends on
 *  - A time interval at which the DStream generates an RDD
 *  - A function that is used to generate an RDD after each time interval
 */

abstract class DStream[T: ClassManifest] (
    @transient protected[streaming] var ssc: StreamingContext
  ) extends Serializable with Logging {

  initLogging()

  // =======================================================================
  // Methods that should be implemented by subclasses of DStream
  // =======================================================================

  /** Time interval after which the DStream generates a RDD */
  def slideDuration: Duration

  /** List of parent DStreams on which this DStream depends on */
  def dependencies: List[DStream[_]]

  /** Method that generates a RDD for the given time */
  def compute (validTime: Time): Option[RDD[T]]

  // =======================================================================
  // Methods and fields available on all DStreams
  // =======================================================================

  // RDDs generated, marked as protected[streaming] so that testsuites can access it
  @transient
  protected[streaming] var generatedRDDs = new HashMap[Time, RDD[T]] ()
  
  // Time zero for the DStream
  protected[streaming] var zeroTime: Time = null

  // Duration for which the DStream will remember each RDD created
  protected[streaming] var rememberDuration: Duration = null

  // Storage level of the RDDs in the stream
  protected[streaming] var storageLevel: StorageLevel = StorageLevel.NONE

  // Checkpoint details
  protected[streaming] val mustCheckpoint = false
  protected[streaming] var checkpointDuration: Duration = null
  protected[streaming] var checkpointData = new DStreamCheckpointData(HashMap[Time, Any]())

  // Reference to whole DStream graph
  protected[streaming] var graph: DStreamGraph = null

  protected[streaming] def isInitialized = (zeroTime != null)

  // Duration for which the DStream requires its parent DStream to remember each RDD created
  protected[streaming] def parentRememberDuration = rememberDuration

  /** Returns the StreamingContext associated with this DStream */
  def context() = ssc

  /** Persists the RDDs of this DStream with the given storage level */
  def persist(level: StorageLevel): DStream[T] = {
    if (this.isInitialized) {
      throw new UnsupportedOperationException(
        "Cannot change storage level of an DStream after streaming context has started")
    }
    this.storageLevel = level
    this
  }

  /** Persists RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) */
  def persist(): DStream[T] = persist(StorageLevel.MEMORY_ONLY_SER)

  /** Persists RDDs of this DStream with the default storage level (MEMORY_ONLY_SER) */
  def cache(): DStream[T] = persist()

  /**
   * Enable periodic checkpointing of RDDs of this DStream
   * @param interval Time interval after which generated RDD will be checkpointed
   */
  def checkpoint(interval: Duration): DStream[T] = {
    if (isInitialized) {
      throw new UnsupportedOperationException(
        "Cannot change checkpoint interval of an DStream after streaming context has started")
    }
    persist()
    checkpointDuration = interval
    this
  }

  /**
   * This method initializes the DStream by setting the "zero" time, based on which
   * the validity of future times is calculated. This method also recursively initializes
   * its parent DStreams.
   */
  protected[streaming] def initialize(time: Time) {
    if (zeroTime != null && zeroTime != time) {
      throw new Exception("ZeroTime is already initialized to " + zeroTime
        + ", cannot initialize it again to " + time)
    }
    zeroTime = time

    // Set the checkpoint interval to be slideDuration or 10 seconds, which ever is larger
    if (mustCheckpoint && checkpointDuration == null) {
      checkpointDuration = slideDuration.max(Seconds(10))
      logInfo("Checkpoint interval automatically set to " + checkpointDuration)
    }

    // Set the minimum value of the rememberDuration if not already set
    var minRememberDuration = slideDuration
    if (checkpointDuration != null && minRememberDuration <= checkpointDuration) {
      minRememberDuration = checkpointDuration * 2  // times 2 just to be sure that the latest checkpoint is not forgetten
    }
    if (rememberDuration == null || rememberDuration < minRememberDuration) {
      rememberDuration = minRememberDuration
    }

    // Initialize the dependencies
    dependencies.foreach(_.initialize(zeroTime))
  }

  protected[streaming] def validate() {
    assert(rememberDuration != null, "Remember duration is set to null")

    assert(
      !mustCheckpoint || checkpointDuration != null,
      "The checkpoint interval for " + this.getClass.getSimpleName + " has not been set." +
        " Please use DStream.checkpoint() to set the interval."
    )

    assert(
     checkpointDuration == null || ssc.sc.checkpointDir.isDefined,
      "The checkpoint directory has not been set. Please use StreamingContext.checkpoint()" +
      " or SparkContext.checkpoint() to set the checkpoint directory."
    )

    assert(
      checkpointDuration == null || checkpointDuration >= slideDuration,
      "The checkpoint interval for " + this.getClass.getSimpleName + " has been set to " +
        checkpointDuration + " which is lower than its slide time (" + slideDuration + "). " +
        "Please set it to at least " + slideDuration + "."
    )

    assert(
      checkpointDuration == null || checkpointDuration.isMultipleOf(slideDuration),
      "The checkpoint interval for " + this.getClass.getSimpleName + " has been set to " +
        checkpointDuration + " which not a multiple of its slide time (" + slideDuration + "). " +
        "Please set it to a multiple " + slideDuration + "."
    )

    assert(
      checkpointDuration == null || storageLevel != StorageLevel.NONE,
      "" + this.getClass.getSimpleName + " has been marked for checkpointing but the storage " +
        "level has not been set to enable persisting. Please use DStream.persist() to set the " +
        "storage level to use memory for better checkpointing performance."
    )

    assert(
      checkpointDuration == null || rememberDuration > checkpointDuration,
      "The remember duration for " + this.getClass.getSimpleName + " has been set to " +
        rememberDuration + " which is not more than the checkpoint interval (" +
        checkpointDuration + "). Please set it to higher than " + checkpointDuration + "."
    )

    val metadataCleanerDelay = spark.util.MetadataCleaner.getDelaySeconds
    logInfo("metadataCleanupDelay = " + metadataCleanerDelay)
    assert(
      metadataCleanerDelay < 0 || rememberDuration.milliseconds < metadataCleanerDelay * 1000,
      "It seems you are doing some DStream window operation or setting a checkpoint interval " +
        "which requires " + this.getClass.getSimpleName + " to remember generated RDDs for more " +
        "than " + rememberDuration.milliseconds + " milliseconds. But the Spark's metadata cleanup" +
        "delay is set to " + (metadataCleanerDelay / 60.0) + " minutes, which is not sufficient. Please set " +
        "the Java property 'spark.cleaner.delay' to more than " +
        math.ceil(rememberDuration.milliseconds.toDouble / 60000.0).toInt + " minutes."
    )

    dependencies.foreach(_.validate())

    logInfo("Slide time = " + slideDuration)
    logInfo("Storage level = " + storageLevel)
    logInfo("Checkpoint interval = " + checkpointDuration)
    logInfo("Remember duration = " + rememberDuration)
    logInfo("Initialized and validated " + this)
  }

  protected[streaming] def setContext(s: StreamingContext) {
    if (ssc != null && ssc != s) {
      throw new Exception("Context is already set in " + this + ", cannot set it again")
    }
    ssc = s
    logInfo("Set context for " + this)
    dependencies.foreach(_.setContext(ssc))
  }

  protected[streaming] def setGraph(g: DStreamGraph) {
    if (graph != null && graph != g) {
      throw new Exception("Graph is already set in " + this + ", cannot set it again")
    }
    graph = g
    dependencies.foreach(_.setGraph(graph))
  }

  protected[streaming] def remember(duration: Duration) {
    if (duration != null && duration > rememberDuration) {
      rememberDuration = duration
      logInfo("Duration for remembering RDDs set to " + rememberDuration + " for " + this)
    }
    dependencies.foreach(_.remember(parentRememberDuration))
  }

  /** This method checks whether the 'time' is valid wrt slideDuration for generating RDD */
  protected def isTimeValid(time: Time): Boolean = {
    if (!isInitialized) {
      throw new Exception (this + " has not been initialized")
    } else if (time <= zeroTime || ! (time - zeroTime).isMultipleOf(slideDuration)) {
      false
    } else {
      true
    }
  }

  /**
   * Retrieves a precomputed RDD of this DStream, or computes the RDD. This is an internal
   * method that should not be called directly.
   */  
  protected[streaming] def getOrCompute(time: Time): Option[RDD[T]] = {
    // If this DStream was not initialized (i.e., zeroTime not set), then do it
    // If RDD was already generated, then retrieve it from HashMap
    generatedRDDs.get(time) match {
      
      // If an RDD was already generated and is being reused, then 
      // probably all RDDs in this DStream will be reused and hence should be cached
      case Some(oldRDD) => Some(oldRDD)
      
      // if RDD was not generated, and if the time is valid
      // (based on sliding time of this DStream), then generate the RDD
      case None => {
        if (isTimeValid(time)) {
          compute(time) match {
            case Some(newRDD) =>
              if (storageLevel != StorageLevel.NONE) {
                newRDD.persist(storageLevel)
                logInfo("Persisting RDD " + newRDD.id + " for time " + time + " to " + storageLevel + " at time " + time)
              }
              if (checkpointDuration != null && (time - zeroTime).isMultipleOf(checkpointDuration)) {
                newRDD.checkpoint()
                logInfo("Marking RDD " + newRDD.id + " for time " + time + " for checkpointing at time " + time)
              }
              generatedRDDs.put(time, newRDD)
              Some(newRDD)
            case None => 
              None
          }
        } else {
          None
        }
      }
    }
  }

  /**
   * Generates a SparkStreaming job for the given time. This is an internal method that
   * should not be called directly. This default implementation creates a job
   * that materializes the corresponding RDD. Subclasses of DStream may override this
   * (eg. ForEachDStream).
   */
  protected[streaming] def generateJob(time: Time): Option[Job] = {
    getOrCompute(time) match {
      case Some(rdd) => {
        val jobFunc = () => {
          val emptyFunc = { (iterator: Iterator[T]) => {} } 
          ssc.sc.runJob(rdd, emptyFunc)
        }
        Some(new Job(time, jobFunc))
      }
      case None => None
    }
  }

  /**
   * Dereferences RDDs that are older than rememberDuration.
   */
  protected[streaming] def forgetOldRDDs(time: Time) {
    val keys = generatedRDDs.keys
    var numForgotten = 0
    keys.foreach(t => {
      if (t <= (time - rememberDuration)) {
        generatedRDDs.remove(t)
        numForgotten += 1
        logInfo("Forgot RDD of time " + t + " from " + this)
      }
    })
    logInfo("Forgot " + numForgotten + " RDDs from " + this)
    dependencies.foreach(_.forgetOldRDDs(time))
  }

  /* Adds metadata to the Stream while it is running. 
   * This methd should be overwritten by sublcasses of InputDStream.
   */
  protected[streaming] def addMetadata(metadata: Any) {
    if (metadata != null) {
      logInfo("Dropping Metadata: " + metadata.toString)
    }
  }

  /**
   * Refreshes the list of checkpointed RDDs that will be saved along with checkpoint of
   * this stream. This is an internal method that should not be called directly. This is
   * a default implementation that saves only the file names of the checkpointed RDDs to
   * checkpointData. Subclasses of DStream (especially those of InputDStream) may override
   * this method to save custom checkpoint data.
   */
  protected[streaming] def updateCheckpointData(currentTime: Time) {
    logInfo("Updating checkpoint data for time " + currentTime)

    // Get the checkpointed RDDs from the generated RDDs
    val newRdds = generatedRDDs.filter(_._2.getCheckpointFile.isDefined)
                               .map(x => (x._1, x._2.getCheckpointFile.get))

    // Make a copy of the existing checkpoint data (checkpointed RDDs)
    val oldRdds = checkpointData.rdds.clone()

    // If the new checkpoint data has checkpoints then replace existing with the new one
    if (newRdds.size > 0) {
      checkpointData.rdds.clear()
      checkpointData.rdds ++= newRdds
    }

    // Make parent DStreams update their checkpoint data
    dependencies.foreach(_.updateCheckpointData(currentTime))

    // TODO: remove this, this is just for debugging
    newRdds.foreach {
      case (time, data) => { logInfo("Added checkpointed RDD for time " + time + " to stream checkpoint") }
    }

    if (newRdds.size > 0) {
      (oldRdds -- newRdds.keySet).foreach {
        case (time, data) => {
          val path = new Path(data.toString)
          val fs = path.getFileSystem(new Configuration())
          fs.delete(path, true)
          logInfo("Deleted checkpoint file '" + path + "' for time " + time)
        }
      }
    }
    logInfo("Updated checkpoint data for time " + currentTime + ", " + checkpointData.rdds.size + " checkpoints, " 
      + "[" + checkpointData.rdds.mkString(",") + "]")
  }

  /**
   * Restores the RDDs in generatedRDDs from the checkpointData. This is an internal method
   * that should not be called directly. This is a default implementation that recreates RDDs
   * from the checkpoint file names stored in checkpointData. Subclasses of DStream that
   * override the updateCheckpointData() method would also need to override this method.
   */
  protected[streaming] def restoreCheckpointData() {
    // Create RDDs from the checkpoint data
    logInfo("Restoring checkpoint data from " + checkpointData.rdds.size + " checkpointed RDDs")
    checkpointData.rdds.foreach {
      case(time, data) => {
        logInfo("Restoring checkpointed RDD for time " + time + " from file '" + data.toString + "'")
        val rdd = ssc.sc.checkpointFile[T](data.toString)
        generatedRDDs += ((time, rdd))
      }
    }
    dependencies.foreach(_.restoreCheckpointData())
    logInfo("Restored checkpoint data")
  }

  @throws(classOf[IOException])
  private def writeObject(oos: ObjectOutputStream) {
    logDebug(this.getClass().getSimpleName + ".writeObject used")
    if (graph != null) {
      graph.synchronized {
        if (graph.checkpointInProgress) {
          oos.defaultWriteObject()
        } else {
          val msg = "Object of " + this.getClass.getName + " is being serialized " +
            " possibly as a part of closure of an RDD operation. This is because " +
            " the DStream object is being referred to from within the closure. " +
            " Please rewrite the RDD operation inside this DStream to avoid this. " +
            " This has been enforced to avoid bloating of Spark tasks " +
            " with unnecessary objects."
          throw new java.io.NotSerializableException(msg)
        }
      }
    } else {
      throw new java.io.NotSerializableException("Graph is unexpectedly null when DStream is being serialized.")
    }
  }

  @throws(classOf[IOException])
  private def readObject(ois: ObjectInputStream) {
    logDebug(this.getClass().getSimpleName + ".readObject used")
    ois.defaultReadObject()
    generatedRDDs = new HashMap[Time, RDD[T]] ()
  }

  // =======================================================================
  // DStream operations
  // =======================================================================

  /** Returns a new DStream by applying a function to all elements of this DStream. */
  def map[U: ClassManifest](mapFunc: T => U): DStream[U] = {
    new MappedDStream(this, ssc.sc.clean(mapFunc))
  }

  /**
   * Returns a new DStream by applying a function to all elements of this DStream,
   * and then flattening the results
   */
  def flatMap[U: ClassManifest](flatMapFunc: T => Traversable[U]): DStream[U] = {
    new FlatMappedDStream(this, ssc.sc.clean(flatMapFunc))
  }

  /** Returns a new DStream containing only the elements that satisfy a predicate. */
  def filter(filterFunc: T => Boolean): DStream[T] = new FilteredDStream(this, filterFunc)

  /**
   * Return a new DStream in which each RDD is generated by applying glom() to each RDD of
   * this DStream. Applying glom() to an RDD coalesces all elements within each partition into
   * an array.
   */
  def glom(): DStream[Array[T]] = new GlommedDStream(this)

  /**
   * Return a new DStream in which each RDD is generated by applying mapPartitions() to each RDDs
   * of this DStream. Applying mapPartitions() to an RDD applies a function to each partition
   * of the RDD.
   */
  def mapPartitions[U: ClassManifest](
      mapPartFunc: Iterator[T] => Iterator[U],
      preservePartitioning: Boolean = false
    ): DStream[U] = {
    new MapPartitionedDStream(this, ssc.sc.clean(mapPartFunc), preservePartitioning)
  }

  /**
   * Returns a new DStream in which each RDD has a single element generated by reducing each RDD
   * of this DStream.
   */
  def reduce(reduceFunc: (T, T) => T): DStream[T] =
    this.map(x => (null, x)).reduceByKey(reduceFunc, 1).map(_._2)

  /**
   * Returns a new DStream in which each RDD has a single element generated by counting each RDD
   * of this DStream.
   */
  def count(): DStream[Long] = this.map(_ => 1L).reduce(_ + _)

  /**
   * Applies a function to each RDD in this DStream. This is an output operator, so
   * this DStream will be registered as an output stream and therefore materialized.
   */
  def foreach(foreachFunc: RDD[T] => Unit) {
    foreach((r: RDD[T], t: Time) => foreachFunc(r))
  }

  /**
   * Applies a function to each RDD in this DStream. This is an output operator, so
   * this DStream will be registered as an output stream and therefore materialized.
   */
  def foreach(foreachFunc: (RDD[T], Time) => Unit) {
    val newStream = new ForEachDStream(this, ssc.sc.clean(foreachFunc))
    ssc.registerOutputStream(newStream)
    newStream
  }

  /**
   * Returns a new DStream in which each RDD is generated by applying a function
   * on each RDD of this DStream.
   */
  def transform[U: ClassManifest](transformFunc: RDD[T] => RDD[U]): DStream[U] = {
    transform((r: RDD[T], t: Time) => transformFunc(r))
  }

  /**
   * Returns a new DStream in which each RDD is generated by applying a function
   * on each RDD of this DStream.
   */
  def transform[U: ClassManifest](transformFunc: (RDD[T], Time) => RDD[U]): DStream[U] = {
    new TransformedDStream(this, ssc.sc.clean(transformFunc))
  }

  /**
   * Prints the first ten elements of each RDD generated in this DStream. This is an output
   * operator, so this DStream will be registered as an output stream and there materialized.
   */
  def print() {
    def foreachFunc = (rdd: RDD[T], time: Time) => {
      val first11 = rdd.take(11)
      println ("-------------------------------------------")
      println ("Time: " + time)
      println ("-------------------------------------------")
      first11.take(10).foreach(println)
      if (first11.size > 10) println("...")
      println()
    }
    val newStream = new ForEachDStream(this, ssc.sc.clean(foreachFunc))
    ssc.registerOutputStream(newStream)
  }

  /**
   * Return a new DStream which is computed based on windowed batches of this DStream.
   * The new DStream generates RDDs with the same interval as this DStream.
   * @param windowDuration width of the window; must be a multiple of this DStream's interval.
   */
  def window(windowDuration: Duration): DStream[T] = window(windowDuration, this.slideDuration)

  /**
   * Return a new DStream which is computed based on windowed batches of this DStream.
   * @param windowDuration width of the window; must be a multiple of this DStream's
   *                       batching interval
   * @param slideDuration  sliding interval of the window (i.e., the interval after which
   *                       the new DStream will generate RDDs); must be a multiple of this
   *                       DStream's batching interval
   */
  def window(windowDuration: Duration, slideDuration: Duration): DStream[T] = {
    new WindowedDStream(this, windowDuration, slideDuration)
  }

  /**
   * Returns a new DStream which computed based on tumbling window on this DStream.
   * This is equivalent to window(batchTime, batchTime).
   * @param batchDuration tumbling window duration; must be a multiple of this DStream's
   *                  batching interval
   */
  def tumble(batchDuration: Duration): DStream[T] = window(batchDuration, batchDuration)

  /**
   * Returns a new DStream in which each RDD has a single element generated by reducing all
   * elements in a window over this DStream. windowDuration and slideDuration are as defined
   * in the window() operation. This is equivalent to
   * window(windowDuration, slideDuration).reduce(reduceFunc)
   */
  def reduceByWindow(
      reduceFunc: (T, T) => T,
      windowDuration: Duration,
      slideDuration: Duration
    ): DStream[T] = {
    this.window(windowDuration, slideDuration).reduce(reduceFunc)
  }

  def reduceByWindow(
      reduceFunc: (T, T) => T,
      invReduceFunc: (T, T) => T,
      windowDuration: Duration,
      slideDuration: Duration
    ): DStream[T] = {
      this.map(x => (1, x))
          .reduceByKeyAndWindow(reduceFunc, invReduceFunc, windowDuration, slideDuration, 1)
          .map(_._2)
  }

  /**
   * Returns a new DStream in which each RDD has a single element generated by counting the number
   * of elements in a window over this DStream. windowDuration and slideDuration are as defined in the
   * window() operation. This is equivalent to window(windowDuration, slideDuration).count()
   */
  def countByWindow(windowDuration: Duration, slideDuration: Duration): DStream[Long] = {
    this.map(_ => 1L).reduceByWindow(_ + _, _ - _, windowDuration, slideDuration)
  }

  /**
   * Returns a new DStream by unifying data of another DStream with this DStream.
   * @param that Another DStream having the same slideDuration as this DStream.
   */
  def union(that: DStream[T]): DStream[T] = new UnionDStream[T](Array(this, that))

  /**
   * Returns all the RDDs defined by the Interval object (both end times included)
   */
  protected[streaming] def slice(interval: Interval): Seq[RDD[T]] = {
    slice(interval.beginTime, interval.endTime)
  }

  /**
   * Returns all the RDDs between 'fromTime' to 'toTime' (both included)
   */
  def slice(fromTime: Time, toTime: Time): Seq[RDD[T]] = {
    val rdds = new ArrayBuffer[RDD[T]]()
    var time = toTime.floor(slideDuration)
    while (time >= zeroTime && time >= fromTime) {
      getOrCompute(time) match {
        case Some(rdd) => rdds += rdd
        case None => //throw new Exception("Could not get RDD for time " + time)
      }
      time -= slideDuration
    }
    rdds.toSeq
  }

  /**
   * Saves each RDD in this DStream as a Sequence file of serialized objects.
   * The file name at each batch interval is generated based on `prefix` and
   * `suffix`: "prefix-TIME_IN_MS.suffix".
   */
  def saveAsObjectFiles(prefix: String, suffix: String = "") {
    val saveFunc = (rdd: RDD[T], time: Time) => {
      val file = rddToFileName(prefix, suffix, time)
      rdd.saveAsObjectFile(file)
    }
    this.foreach(saveFunc)
  }

  /**
   * Saves each RDD in this DStream as at text file, using string representation
   * of elements. The file name at each batch interval is generated based on
   * `prefix` and `suffix`: "prefix-TIME_IN_MS.suffix".
   */
  def saveAsTextFiles(prefix: String, suffix: String = "") {
    val saveFunc = (rdd: RDD[T], time: Time) => {
      val file = rddToFileName(prefix, suffix, time)
      rdd.saveAsTextFile(file)
    }
    this.foreach(saveFunc)
  }

  def register() {
    ssc.registerOutputStream(this)
  }
}

private[streaming]
case class DStreamCheckpointData(rdds: HashMap[Time, Any])

