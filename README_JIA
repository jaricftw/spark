I. How to build spark with spark-sql enabled:
    mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.0 -Phive -Phive-thriftserver -DskipTests clean package

II. How to build a runnable distribution with spark-sql enabled:
    ./make-distribution.sh --name custom-spark --tgz -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.0 -Phive -Phive-thriftserver
    Then a dist/ folder will be generated and can be copied to different nodes for execution

III. How to view job history on UI after fact:
    0. Enable history log, and specify the history server port, otherwise default: 18080
        spark.eventLog.enabled true
        spark.history.ui.port 11510

    1. Start a history server
    ./sbin/start-history-server.sh

    2. Go to http://[master-url]:11510

    misc: the default directory to store history logs: /tmp/spark-events; 
          Only one history server can be launched at one time. 
          Stop it by ./sbin/stop-history-server.sh beore launching a new one


IV. How to setup Spark cluster at ec2-cluster:
    Instructions: http://spark.apache.org/docs/latest/ec2-scripts.html
    Note: (1) by default it uses m1.large which incurs fees. To change this, use --instance-type
          (2) by default, it downloads spark from apache github page, and the default branch is 1.5.0, 
                this can be changes by specifying --spark-git-repo and --spark-version

    As an example, here is how to setup a cluster with three slaves:
    ./spark-ec2 -k initial-trial-key -i ../../../AWS/initial-trial-key.pem -s 3 --region=us-west-2 --zone=us-west-2a launch test

    The following instructions set up a three-slave cluster called "testm4" with m4.2xlarge instances
    (a). Here is how to setup a cluster with specific instances and spark versions:
    ./spark-ec2 -k initial-trial-key -i ../../../AWS/initial-trial-key.pem -s 3 --region=us-west-2 --zone=us-west-2a --instance-type=m4.2xlarge --spark-git-repo=https://github.com/jaricftw/spark --spark-version=1.5.1 launch testm4

    (b). Here is how to login to the master
    ./spark-ec2 -k initial-trial-key -i ../../../AWS/initial-trial-key.pem -s 3 --region=us-west-2 --zone=us-west-2a login testm4 

    (c). Here is the webUI:
    http://ec2-54-201-14-78.us-west-2.compute.amazonaws.com:8080/
   
    (d). Ganlia monitor webUI:
    http://ec2-54-201-14-78.us-west-2.compute.amazonaws.com:5080/ganglia/

    (e). Stop the cluster:
    ./spark-ec2 --region=us-west-2 stop testm4

    (f). Destroy the cluster:
    ./spark-ec2 destroy <cluster-name>

    Note that, spark-sql is not built with the default script. We need to modify the script to build a runnable distribution with spark-sql enabled, just like II
  
 




